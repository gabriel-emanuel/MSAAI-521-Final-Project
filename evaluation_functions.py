# -*- coding: utf-8 -*-
"""Evaluation_Functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HqxECscJ3GV3LVsjh-SsbnNvysQliPM
"""
version = "0.0.3"

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.applications.densenet import preprocess_input

print(f"evaluation_functions version: {version} is loaded.\n")

# Draw the learning curve(s) based on training history
# primarily compares traning and validation curves

def group5_display_lc(history={}, metrics=[], train_val = 'both', model_name = []):
  # history: result after fitting model on training (and validation) dataset
  # metrics:
  #   > [(empty)] (default): plot learning curves for all metrics available in history
  #   > [non-empty] : plot learning curves for specified metrics in this list
  # train_val:
  #   > 'both' (default): overlays train learning curve on validation learning curve for a given metric
  #   > 'train' : displays learning curves on training set only (not implemented)
  #   > 'val' : displays learning curves on validation set only (not implemented)

  # print metrics common to both train and val sets available in 'history'
  available_metrics = [key for key in history.history.keys() if not key.startswith('val_')]
  print(f"Available Metrics1: {available_metrics}\n")

  if (len(metrics) == 0):
    metrics = np.copy(available_metrics)
    print(f"Here1: metrics: {metrics}\n")
  # print selected metrics to display
  print(f"Metrics to display: {metrics}\n")

  num_metrics = len(metrics)
  max_cols = 2
  num_rows = np.ceil(num_metrics/max_cols).astype(int)
  print("Num Rows: ", num_rows)

  # plot learning curves for each metric listed in metrics
  plt.figure(figsize=(10,4))
  for i, metric in enumerate(metrics):
    print(f"plotA {i} train/val curves for metric {metric}\n")

    # Draw the learning curves and write down your understanding from the graphs.
    plt.subplot(num_rows,max_cols,i+1)
    plt.plot(history.history[metric])
    plt.plot(history.history['val_'+metric])
    plt.title(model_name+': '+metric)
    plt.ylabel(metric)
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='lower right')
    plt.minorticks_on()
    plt.grid(which='major', axis ='y')
    plt.grid(which='both', axis='x')
  
  plt.show()



  return

def group5_densenet_preprocess(image):
    image = preprocess_input(image)
    return image


# predict on test set, return y_true, y_pred labels
def group5_predict_test_set(test_dir, model, class_list=[], batch_size = 32, image_size=(256,256)):
  # always evaluated on test_dataset
  # load test dataset
  test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=image_size,      
    batch_size=batch_size,
    labels='inferred',           # Automatically infer labels from subdirectory names
    label_mode='int',
    #label_mode = 'categorical',
    class_names=class_list
  )
  
  # Create a Rescaling layer (use preprocessing by Densenet)
  rescaling_layer = tf.keras.layers.Rescaling(scale=1./255)
  # test_datset = test_dataset.map(group5_densenet_preprocess)

  # initialize true and predicted labels array
  true_labels = []
  pred_labels = []

  # iterate through images in the test_dataset and get predictions
  for images, labels in test_dataset:
    #print(f"images shape: {images.shape}")
    #images = rescaling_layer(images)
    images = preprocess_input(images)
    #print(f"scaled images shape: {scaled_images.shape}")

    # get model predictions for this batch
    pred_probs = model.predict(images)

    #print(f"PRED PROBS SHAPE:{pred_probs.shape}")
    preds = np.argmax(pred_probs, axis=1)

    #print(f"PREDS SHAPE:{preds.shape}")

    # append results
    true_labels.extend(labels)
    pred_labels.extend(preds)

  # return pred and true values
  return pred_labels, true_labels

# create a single functions that will go through the entire evaluation process

# display confusion matrix
def group5_confusion_matrix(y_true, y_pred, class_list=[]):
  cm = confusion_matrix(y_true, y_pred)
  cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

  plt.figure(figsize=(10, 8))
  sns.heatmap(cm_normalized, xticklabels = class_list, yticklabels = class_list, annot=True, fmt='.2f', cmap='Blues')
  plt.title('Normalized Confusion Matrix')
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')
  plt.show()


def group5_evaluate(TEST_DIR, model=[], model_name="default",class_list =[], 
training_history = [], batch_size=32, image_size = (256,256) ):

  # STEP 1 Display Learning Curves during Training
  group5_display_lc(training_history, model_name = model_name)

  # STEP 2 Make Predictions on Test Set and get true labels
  pred_labels, true_labels = group5_predict_test_set(TEST_DIR, model,class_list=class_list, batch_size = 256, image_size = image_size )

  # STEP 3 Print Classification Report
  print(classification_report(true_labels, pred_labels, target_names = class_list))

  # STEP 4 Display Confusion Matrix Normalized
  group5_confusion_matrix(true_labels, pred_labels, class_list)
